This is a work in progress and will eventually be converted to a more readable
format.

TraceR is a replay tool targeted to simulate control flow of application on
prototype systems, i.e., if control flow of an application, which includes
expected computation tasks, communication routines, and their dependencies, is
provided to TraceR, it will mimic the flow on a hypothetical system with a given
compute and communication capability. As of now, the control flow is captured by
either emulating applications using BigSim or by linking with Score-P. CODES
is used for simulating the communication on the network.

Expected work flow:

1) Write an MPI application. (Avoid global variables so that the application be
run with virtualization if using BigSim).

If using BigSim follows steps 2-4, else follow step 5.
2) Compile BigSim/Charm++ for emulation. Use any one of the following commands:

- To use UDP as BigSim/Charm++'s communication layer:
  ./build bgampi net-linux-x86_64 bigemulator --with-production --enable-tracing
  ./build bgampi net-darwin-x86_64 bigemulator --with-production --enable-tracing

  or explicitly provide the compiler optimization level
  ./build bgampi net-linux-x86_64 bigemulator -O2

- To use MPI as BigSim/Charm++'s communication layer:
  ./build bgampi mpi-linux-x86_64 bigemulator --with-production --enable-tracing

Note that this build is used to compile MPI applications so that traces can be
generated. Hence, the communication layer used by BigSim/Charm++ is not
important.  During simulation, the communication will be replayed using the
network simulator from CODES. However, the computation time captured here can be
important if it is not being explicitly replaced at simulation time using
configuration options. So using appropriate compiler flags is important.

3) Compile the MPI application from Step 1 using BigSim/Charm++ from Step 2.

Example commands:
$CHARM_DIR/bin/ampicc -O2 simplePrg.c -o simplePrg_c
$CHARM_DIR/bin/ampiCC -O2 simplePrg.cc -o simplePrg_cxx

4) Emulation to generate traces. When the binary generated in Step 3 is run,
BigSim/Charm++ runs the program on the allocated cores as if it would run in the
usual case. Users should provide a few additional arguments to specify the
number of MPI processes in the prototype systems.

If using UDP as the BigSim/Charm++'s communication layer:
./charmrun +p<number of real processes> ++nodelist <machine file> ./pgm <program arguments> +vp<number of processes expected on the future system> +x<x dim> +y<y dim> +z<z dim> +bglog

If using MPI as the BigSim/Charm++'s communication layer:
mpirun -n<number of real processes> ./pgm <program arguments> +vp<number of processes expected on the future system> +x<x dim> +y<y dim> +z<z dim> +bglog

Number of real processes is typically equal to the number cores the emulation
is being run on.

machine file is the list of systems the emulation should be run on (similar to
machine file for MPI; refer to Charm++ website for more details).

vp is the number of MPI ranks that are to be emulated. For simple tests, it can
be same as the number of real processes, in which case one MPI rank is run on
each real processes (as it happens when a regular program is run). When the
number of vp (virtual processes) is higher, BigSim launches user level threads
to execute multiple MPI ranks with a process.

+x +y +z defines a 3D grid of the virtual processes. The product of these three
dimensions must match the number of vp's. These arguments do not have any
effect on the emulation, but exist due to historical reasons.

+bglog instructs bigsim to write the logs to files.

When this run finished, you should see many files named bgTrace* in the
directory. The total number of such files equals the number of real processes
plus one. Their names are bgTrace, bgTrace0, bgTrace1, so on.

Create a new folder and move all bgTrace to that folder.

5) Following instructions in README.OTF to generate OTF2 traces.

6) Simulation. To run a simulation, 2 files are needed: a tracer config file,
and a codes config file. Optionally, mapping files can also be provided.

Tracer config file: sample found at examples/jacobi2d-bigsim/tracer_config (BigSim) or examples/stencil4d-otf/tracer_config (OTF) Format (expected content on each line of the file):
<global_map file>
<num jobs>
<Trace path for job0> <map file for job0> <number of ranks in job0> <iterations (use 1 if running in normal mode)>
<Trace path for job1> <map file for job1> <number of ranks in job1> <iterations (use 1 if running in normal mode)>
...
```
If <global_map file> is not needed, use NA for it and <map file for job*>.
For generating simple global and job map file, use the code in utils.

CODES config files: samples in examples/conf

Additional documentation on format of the CODES config file can be found in the
CODES wiki at https://xgitlab.cels.anl.gov/codes/codes/wikis/home

Brief summary follows:

LPGROUPS, MODELNET_GRP, PARAMS are keywords and should be used as is.

MODELNET_GRP:
repetition = number of routers that have nodes connecting to them.

server = number of MPI processes/cores per router

modelnet_* = number of NICs. For torus, this value has to be 1; for dragonfly,
it should be router radix divided by 4; for the fat-tree, it should be router
radix divided by 2. For the dragonfly network, modelnet_dragonfly_router should
also be specified (as 1). For express mesh, modelnet_express_mesh_router should
also be specified as 1.

Similarly, the fat-tree config file requires specifying fattree_switch which
can be 2 or 3, depending on the number of levels in the fat-tree. Note that the
total number of cores specified in the CODES config file can be greater than
the number of MPI processes being simulated (specified in the tracer config
file).

Other common parameters:
packet_size/chunk_size (both should have the same value): size of the packets
created by NIC for transmission on the network. Smaller the packet size, longer
the time for which simulation will run (in real time). Larger the packet size,
the less accurate the predictions are expected to be (in virtual time). Packet
sizes of 512 bytes to 4096 bytes are commonly used.

modelnet_order = torus/dragonfly/fattree/slimfly/express_mesh

modelnet_scheduler =
fcfs : packetize messages one by one.
round-robin : packetize message in a round robin manner.

message_size = PDES parameter (keep constant at 512)

router_delay = delay at each router for packet transmission (in nano seconds)

soft_delay = delay caused by software stack such as that of MPI (in nano
seconds)

link_bandwidth = bandwidth of each link in the system (in GB/s)

cn_bandwidth = bandwidth of connection between NIC and router (in GB/s)

buffer_size/vc_size = size of channels used to store transient packets at routers (in
bytes). Typical value is 64*packet_size.

routing = how are packets being routed. Options depend on the network:
torus = static/adaptive
dragonfly = minimal/nonminimal/adaptive
fat-tree = adaptive/static

Network specific parameters:

Torus: n_dims - number of dimensions in the torus
dim_length - length of each dimension

Dragonfly: num_routers - number of routers within a group.
global_bandwidth - bandwidth of the links that connect groups.

Fat-tree: ft_type - always choose 1
num_levels - number of levels in the fat-tree (2 or 3)
switch_radix -  radix of the switch being used
switch_count - number of switches at leaf level.

Publications that describe implementation of TraceR in detail:
Nikhil Jain, Abhinav Bhatele, Sam White, Todd Gamblin, and Laxmikant Kale.
Evaluating HPC Networks via Simulation of Parallel Workloads. SC 2016.

Bilge Acun, Nikhil Jain, Abhinav Bhatele, Misbah Mubarak, Christopher Carothers,
Laxmikant Kale. Preliminary Evaluation of a Parallel Trace Replay Tool for HPC
Network Simulations. Workshop on Parallel and Distributed Agent-Based
Simulations at EURO-PAR 2015.

More details can be found in Chapter 5 of this thesis:
http://charm.cs.illinois.edu/newPapers/16-02/Jain_Thesis.pdf
